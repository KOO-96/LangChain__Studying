{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9105fe11",
   "metadata": {},
   "source": [
    "# LangChain + HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6f7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "# pip install langchain\n",
    "# pip install huggingface_hub transformers datasets --user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa140e42",
   "metadata": {},
   "source": [
    "### Huggingface Token 발급\n",
    "\n",
    "1. [허깅페이스](https://huggingface.co)에 로그인/회원가입 후 토큰 발급을 신청\n",
    "    - [토큰 발급](https://huggingface.co/docs/hub/security-tokens)\n",
    "        - 토큰 발급을 진행합니다. <img src=\"https://raw.githubusercontent.com/KOO-96/LangChain__Studying/main/image/img4.png\" width=\"50%\"> 만약 버튼이 비활성화 상태일 경우 메일 인증이 안되어있는 경우입니다\n",
    "    - [LLM Key](https://huggingface.co/settings/tokens)를 복사해서 사용합니다.\n",
    "        <img src=\"https://raw.githubusercontent.com/KOO-96/LangChain__Studying/main/image/img5.png\">\n",
    "        - Key 찾는 법은 아래에 있습니다.\n",
    "        <img src=\"https://raw.githubusercontent.com/KOO-96/LangChain__Studying/main/image/img6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae25146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "565a776b",
   "metadata": {},
   "source": [
    "### HuggingFaceHub에 배포된 모델 inference\n",
    "\n",
    "- 추론에 활용할 모델 선택하기\n",
    "    - [허깅페이스 LLM 리더보드](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)를 통해서 모델 리스트와 성능을 확인하는 방법\n",
    "    <img src=\"https://raw.githubusercontent.com/KOO-96/LangChain__Studying/main/image/img7.png\" width=\"70%\">\n",
    "    \n",
    "- HuggingFaceHub\n",
    "```\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# HuggingFace Repository ID\n",
    "repo_id = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "# 질의내용\n",
    "question = \"Who is Son Heung Min?\"\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# HuggingFaceHub 객체 생성\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    "    model_kwargs={\"temperature\": 0.2, \n",
    "                  \"max_length\": 128}\n",
    ")\n",
    "\n",
    "# LLM Chain 객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 실행\n",
    "print(llm_chain.run(question=question))\n",
    "```\n",
    "\n",
    "※ HuggingFace를 사용할때, 용량 확인이 필요합니다!! 추가적으로 토큰 발급받을 때, Write권한으로 하는 것이 불러오기 편합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb89a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Son Heung Min?\n",
      "\n",
      "Answer: 손흥민\n",
      "\n",
      "Son Heung Min is a South Korean footballer who plays as a forward for Tottenham Hotspur in the English Premier League. He is the first Asian player to score 50 goals in the Premier League and is considered one of the best Asian footballers of all time.\n",
      "\n",
      "Son Heung Min was born on July 8, 1992, in Chuncheon, South Korea. He started his\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "\n",
    "# HuggingFace Repository ID\n",
    "repo_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "\n",
    "# 질의내용\n",
    "question = \"Who is Son Heung Min?\"\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# HuggingFaceHub 객체 생성\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    "    model_kwargs={\"temperature\": 0.2, \n",
    "                  \"max_length\": 128}\n",
    ")\n",
    "\n",
    "# LLM Chain 객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 실행\n",
    "print(llm_chain.run(question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78109d22",
   "metadata": {},
   "source": [
    "- 모델을 직접 다운로드 후 로컬(local)에서 추론\n",
    "    - 위 방법은 서버에서 선택된 모델을 추론하고 답변을 반환하는 방식입니다.\n",
    "    - 추론 방식이 간편하게 사용이 가능하지만 성능에 따라 속도가 오래 걸려 Timeout 에러가 발생할 가능성이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36940716",
   "metadata": {},
   "source": [
    "#### GPU 서버를 탑재하거나 Colab에서 Gpu 부스트를 받아서 사용이 가능하기도 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40297ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 허깅페이스 모델/토크나이저를 다운로드 받을 경로\n",
    "os.environ['HF_HOME'] = 'LLM 모델을 다운로드 받을 경로'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80309c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace Model ID\n",
    "model_id = 'beomi/llama-2-ko-7b'\n",
    "\n",
    "# HuggingFacePipeline 객체 생성\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id, \n",
    "    device=0,               # -1: CPU(default), 0번 부터는 CUDA 디바이스 번호 지정시 GPU 사용하여 추론\n",
    "    task=\"text-generation\", # 텍스트 생성\n",
    "    model_kwargs={\"temperature\": 0.1, \n",
    "                  \"max_length\": 64},\n",
    ")\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"질문: {question}\n",
    "\n",
    "답변: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# LLM Chain 객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
